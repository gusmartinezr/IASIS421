{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "odjFlzWLOA6O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pElCCA7gcVq",
        "outputId": "fe4f44fa-aa7f-4f65-c4f4-3f2fb9ee3063"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"/content/drive/MyDrive/IA2/Cuentos.txt\", \"r\", encoding='utf-8')\n",
        "text = f.read()\n",
        "text[:300], len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9sutS20geK6",
        "outputId": "7a06667e-ddcc-411f-c9c5-a087c3100577"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('El águila, el cuervo y el pastor\\n\\nLanzándose desde una cima, un águila arrebató a un corderito.\\nLa vio un cuervo y tratando de imitar al águila, se lanzó sobre un\\ncarnero, pero con tan mal conocimiento en el arte que sus garras se\\nenredaron en la lana, y batiendo al máximo sus alas no logró\\nsoltarse',\n",
              " 3449)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenización"
      ],
      "metadata": {
        "id": "q_QEeO7-hKlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "all_characters = string.printable + \"ñÑáÁéÉíÍóÓúÚ¿¡\"\n",
        "all_characters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ivnQEM5qg-Kz",
        "outputId": "b815df7c-ad78-4824-e992-39e5856e9560"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0cñÑáÁéÉíÍóÓúÚ¿¡'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "class Tokenizer(): \n",
        "    \n",
        "  def __init__(self):\n",
        "    self.all_characters = all_characters\n",
        "    self.n_characters = len(self.all_characters)\n",
        "    \n",
        "  def text_to_seq(self, string):\n",
        "    seq = []\n",
        "    for c in range(len(string)):\n",
        "        try:\n",
        "            seq.append(self.all_characters.index(string[c]))\n",
        "        except:\n",
        "            continue\n",
        "    return seq\n",
        "\n",
        "  def seq_to_text(self, seq):\n",
        "    text = ''\n",
        "    for c in range(len(seq)):\n",
        "        text += self.all_characters[seq[c]]\n",
        "    return text\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.n_characters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE3Ll9DthOsn",
        "outputId": "0e6837d5-1293-4949-945f-4787855c3cae"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "114"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.text_to_seq('El águila')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uVpwuV8hV57",
        "outputId": "1630c6b3-f83e-415c-ae64-506907cd1eb9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[40, 21, 94, 102, 16, 30, 18, 21, 10]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.seq_to_text([40, 21, 94, 102, 16, 30, 18, 21, 10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "v_n-kYa2hi7s",
        "outputId": "cf0daaf4-c27a-4c2c-8776-fb9118246397"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'El águila'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_encoded = tokenizer.text_to_seq(text)"
      ],
      "metadata": {
        "id": "v12lmxJrhu3i"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = len(text_encoded) * 80 // 100 \n",
        "train = text_encoded[:train_size]\n",
        "test = text_encoded[train_size:]\n",
        "\n",
        "len(train), len(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ys94jR9kh0YA",
        "outputId": "091eb025-0481-47c0-8faa-99b5c919b8cc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2759, 690)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def windows(text, window_size = 100):\n",
        "    start_index = 0\n",
        "    end_index = len(text) - window_size\n",
        "    text_windows = []\n",
        "    while start_index < end_index:\n",
        "      text_windows.append(text[start_index:start_index+window_size+1])\n",
        "      start_index += 1\n",
        "    return text_windows\n",
        "\n",
        "text_encoded_windows = windows(text_encoded)"
      ],
      "metadata": {
        "id": "s9P6P0zziCVX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.seq_to_text((text_encoded_windows[0])))\n",
        "print()\n",
        "print(tokenizer.seq_to_text((text_encoded_windows[1])))\n",
        "print()\n",
        "print(tokenizer.seq_to_text((text_encoded_windows[2])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pp95JOUiOMT",
        "outputId": "6b834386-a60f-4ff4-b707-4f72f9095afc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El águila, el cuervo y el pastor\n",
            "\n",
            "Lanzándose desde una cima, un águila arrebató a un corderito.\n",
            "La vi\n",
            "\n",
            "l águila, el cuervo y el pastor\n",
            "\n",
            "Lanzándose desde una cima, un águila arrebató a un corderito.\n",
            "La vio\n",
            "\n",
            " águila, el cuervo y el pastor\n",
            "\n",
            "Lanzándose desde una cima, un águila arrebató a un corderito.\n",
            "La vio \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class CharRNNDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, text_encoded_windows, train=True):\n",
        "    self.text = text_encoded_windows\n",
        "    self.train = train\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.text)\n",
        "\n",
        "  def __getitem__(self, ix):\n",
        "    if self.train:\n",
        "      return torch.tensor(self.text[ix][:-1]), torch.tensor(self.text[ix][-1])\n",
        "    return torch.tensor(self.text[ix])"
      ],
      "metadata": {
        "id": "u6PDS7bCiSf1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text_encoded_windows = windows(train)\n",
        "test_text_encoded_windows = windows(test)\n",
        "\n",
        "dataset = {\n",
        "    'train': CharRNNDataset(train_text_encoded_windows),\n",
        "    'val': CharRNNDataset(test_text_encoded_windows)\n",
        "}\n",
        "\n",
        "dataloader = {\n",
        "    'train': torch.utils.data.DataLoader(dataset['train'], batch_size=64, shuffle=True, pin_memory=True),\n",
        "    'val': torch.utils.data.DataLoader(dataset['val'], batch_size=50, shuffle=False, pin_memory=True),\n",
        "}\n",
        "\n",
        "len(dataset['train']), len(dataset['val'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NP14AhEjigON",
        "outputId": "0d4b2a0d-5c92-41ae-aac2-30330afe2f74"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2659, 590)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input, output = dataset['train'][1]\n",
        "tokenizer.seq_to_text(input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "s0FKI5h6ilSZ",
        "outputId": "e5030f12-d946-472e-92bc-8f129152fdf0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'l águila, el cuervo y el pastor\\n\\nLanzándose desde una cima, un águila arrebató a un corderito.\\nLa vi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.seq_to_text([output])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "siCxGwjOioJ6",
        "outputId": "4d694641-5513-4181-c37f-cf926544bca6"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'o'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CharRNN(torch.nn.Module):\n",
        "  def __init__(self, input_size, embedding_size=100, hidden_size=300, num_layers=2, dropout=0.5):\n",
        "    super().__init__()\n",
        "    self.encoder = torch.nn.Embedding(input_size, embedding_size)\n",
        "    self.rnn = torch.nn.GRU(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
        "    self.fc = torch.nn.Linear(hidden_size, input_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    x, h = self.rnn(x)         \n",
        "    y = self.fc(x[:,-1,:])\n",
        "    return y"
      ],
      "metadata": {
        "id": "EJTtEws0isUZ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iGU_ZH5eN_1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CharRNN(input_size=tokenizer.n_characters)\n",
        "outputs = model(torch.randint(0, tokenizer.n_characters, (64, 50)))\n",
        "outputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EW-oI3vmivcq",
        "outputId": "b3c721ff-84a2-472c-f15c-98d815b1b7e4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 114])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def fit(model, dataloader, epochs=10):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        train_loss = []\n",
        "        bar = tqdm(dataloader['train'])\n",
        "        for batch in bar:\n",
        "            X, y = batch\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_hat = model(X)\n",
        "            loss = criterion(y_hat, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss.append(loss.item())\n",
        "            bar.set_description(f\"loss {np.mean(train_loss):.5f}\")\n",
        "        bar = tqdm(dataloader['val'])\n",
        "        val_loss = []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in bar:\n",
        "                X, y = batch\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                y_hat = model(X)\n",
        "                loss = criterion(y_hat, y)\n",
        "                val_loss.append(loss.item())\n",
        "                bar.set_description(f\"val_loss {np.mean(val_loss):.5f}\")\n",
        "        print(f\"Epoch {epoch}/{epochs} loss {np.mean(train_loss):.5f} val_loss {np.mean(val_loss):.5f}\")\n",
        "\n",
        "def predict(model, X):\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        X = torch.tensor(X).to(device)\n",
        "        pred = model(X.unsqueeze(0))\n",
        "        return pred"
      ],
      "metadata": {
        "id": "xeRZfzEiixMs"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CharRNN(input_size=tokenizer.n_characters)\n",
        "fit(model, dataloader, epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7shauBY4iz-T",
        "outputId": "c93a4b1c-5143-45b0-e330-8cf225c5daca"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 3.34539: 100%|██████████| 42/42 [00:45<00:00,  1.08s/it]\n",
            "val_loss 3.23678: 100%|██████████| 12/12 [00:02<00:00,  4.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 loss 3.34539 val_loss 3.23678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 3.08937: 100%|██████████| 42/42 [00:42<00:00,  1.02s/it]\n",
            "val_loss 3.07907: 100%|██████████| 12/12 [00:02<00:00,  4.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 loss 3.08937 val_loss 3.07907\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 2.94199: 100%|██████████| 42/42 [00:42<00:00,  1.02s/it]\n",
            "val_loss 2.89930: 100%|██████████| 12/12 [00:02<00:00,  4.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 loss 2.94199 val_loss 2.89930\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 2.70537: 100%|██████████| 42/42 [00:42<00:00,  1.02s/it]\n",
            "val_loss 2.68001: 100%|██████████| 12/12 [00:02<00:00,  4.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20 loss 2.70537 val_loss 2.68001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 2.50442: 100%|██████████| 42/42 [00:42<00:00,  1.02s/it]\n",
            "val_loss 2.55933: 100%|██████████| 12/12 [00:02<00:00,  4.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20 loss 2.50442 val_loss 2.55933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 2.36207: 100%|██████████| 42/42 [00:42<00:00,  1.02s/it]\n",
            "val_loss 2.42024: 100%|██████████| 12/12 [00:02<00:00,  4.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20 loss 2.36207 val_loss 2.42024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 2.22094: 100%|██████████| 42/42 [00:44<00:00,  1.06s/it]\n",
            "val_loss 2.37463: 100%|██████████| 12/12 [00:02<00:00,  4.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/20 loss 2.22094 val_loss 2.37463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 2.10144: 100%|██████████| 42/42 [00:42<00:00,  1.02s/it]\n",
            "val_loss 2.38071: 100%|██████████| 12/12 [00:02<00:00,  4.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/20 loss 2.10144 val_loss 2.38071\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.99300: 100%|██████████| 42/42 [00:42<00:00,  1.02s/it]\n",
            "val_loss 2.35179: 100%|██████████| 12/12 [00:02<00:00,  4.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/20 loss 1.99300 val_loss 2.35179\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.90044: 100%|██████████| 42/42 [00:42<00:00,  1.02s/it]\n",
            "val_loss 2.31868: 100%|██████████| 12/12 [00:02<00:00,  4.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/20 loss 1.90044 val_loss 2.31868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.80797: 100%|██████████| 42/42 [00:42<00:00,  1.02s/it]\n",
            "val_loss 2.34924: 100%|██████████| 12/12 [00:02<00:00,  4.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20 loss 1.80797 val_loss 2.34924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.69453: 100%|██████████| 42/42 [00:44<00:00,  1.06s/it]\n",
            "val_loss 2.37876: 100%|██████████| 12/12 [00:02<00:00,  4.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/20 loss 1.69453 val_loss 2.37876\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.59887: 100%|██████████| 42/42 [00:42<00:00,  1.02s/it]\n",
            "val_loss 2.38724: 100%|██████████| 12/12 [00:02<00:00,  4.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/20 loss 1.59887 val_loss 2.38724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.49947: 100%|██████████| 42/42 [00:42<00:00,  1.02s/it]\n",
            "val_loss 2.42705: 100%|██████████| 12/12 [00:02<00:00,  4.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/20 loss 1.49947 val_loss 2.42705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.42207: 100%|██████████| 42/42 [00:42<00:00,  1.02s/it]\n",
            "val_loss 2.43441: 100%|██████████| 12/12 [00:02<00:00,  4.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/20 loss 1.42207 val_loss 2.43441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.33784: 100%|██████████| 42/42 [00:42<00:00,  1.02s/it]\n",
            "val_loss 2.49419: 100%|██████████| 12/12 [00:02<00:00,  4.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/20 loss 1.33784 val_loss 2.49419\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.23380: 100%|██████████| 42/42 [00:42<00:00,  1.02s/it]\n",
            "val_loss 2.56266: 100%|██████████| 12/12 [00:02<00:00,  4.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/20 loss 1.23380 val_loss 2.56266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.17402: 100%|██████████| 42/42 [00:44<00:00,  1.06s/it]\n",
            "val_loss 2.60758: 100%|██████████| 12/12 [00:02<00:00,  4.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/20 loss 1.17402 val_loss 2.60758\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.10539: 100%|██████████| 42/42 [00:42<00:00,  1.02s/it]\n",
            "val_loss 2.70002: 100%|██████████| 12/12 [00:02<00:00,  4.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/20 loss 1.10539 val_loss 2.70002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.01093: 100%|██████████| 42/42 [00:42<00:00,  1.02s/it]\n",
            "val_loss 2.74127: 100%|██████████| 12/12 [00:02<00:00,  4.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/20 loss 1.01093 val_loss 2.74127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = \"El águila, el cuervo y el \"\n",
        "X_new_encoded = tokenizer.text_to_seq(X_new)\n",
        "y_pred = predict(model, X_new_encoded)\n",
        "y_pred = torch.argmax(y_pred, axis=1)[0].item()\n",
        "tokenizer.seq_to_text([y_pred])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Uqz44YVyi2OD",
        "outputId": "edc790a6-caf0-463f-d98a-a17f3ed3aabd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'p'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "  X_new_encoded = tokenizer.text_to_seq(X_new[-100:])\n",
        "  y_pred = predict(model, X_new_encoded)\n",
        "  y_pred = torch.argmax(y_pred, axis=1)[0].item()\n",
        "  X_new += tokenizer.seq_to_text([y_pred])\n",
        "\n",
        "X_new"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "YlP4bafbjLnr",
        "outputId": "ae6a5da6-94da-4c1a-f169-cb54fc3ce0b2"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'El águila, el cuervo y el perro\\n\\nEl caballo lobo en el perro\\n\\nEl caballo lobo en el perro\\n\\nEl caballo lo hobres alos por la galondidiento peri, la garcarse. Perie.v a nidre por las habres alresó\\n¡ lúi diente ente bien la fle ha de las garras de los garras de sus garras de las garras de las garras de sus prospecie el perro.\\n-'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp=1\n",
        "for i in range(100):\n",
        "  X_new_encoded = tokenizer.text_to_seq(X_new[-100:])\n",
        "  y_pred = predict(model, X_new_encoded)\n",
        "  y_pred = y_pred.view(-1).div(temp).exp()\n",
        "  top_i = torch.multinomial(y_pred, 1)[0]\n",
        "  predicted_char = tokenizer.all_characters[top_i]\n",
        "  X_new += predicted_char\n",
        "\n",
        "print(X_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmYzqwmpjN4e",
        "outputId": "ca3e681d-5ddc-48dc-99ee-6c976e32d091"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El águila, el cuervo y el perro\n",
            "\n",
            "El caballo lobo en el perro\n",
            "\n",
            "El caballo lobo en el perro\n",
            "\n",
            "El caballo lo hobres alos por la galondidiento peri, la garcarse. Perie.v a nidre por las habres alresó\n",
            "¡ lúi diente ente bien la fle ha de las garras de los garras de sus garras de las garras de las garras de sus prospecie el perro.\n",
            "- YTú diisigomo mí fierro en lagó ba ubos puro, y él zarpa- ¡ue suy agarrades de los dioses la llegó \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ExHHoJi33cAa"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}